{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bce22b-d5dd-426b-b831-24d35065d0f9",
   "metadata": {},
   "source": [
    "# Inference API\n",
    "\n",
    "Now we have trained the model, then we could use it in our Inference API service, thus, when a new trasaction/review comes to your system, you can predict if it's Fraud or not with it!\n",
    "\n",
    "This is the arch and workflow of it:\n",
    "\n",
    "![GraphSAGE_FraudDetection_Inference](https://user-images.githubusercontent.com/1651790/182292372-2bef1e38-db4e-4949-8f66-bff361ee93d9.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b9bb0-14b0-474e-b8a2-00a575cfa78e",
   "metadata": {},
   "source": [
    "## When A trasaction/review come\n",
    "\n",
    "Now, let's imagine we have this new record, with vertex id: `2048`.\n",
    "\n",
    "In `0. Insert Record` phase, it's already inserted into the Graph, together with the edges connected with it.\n",
    "\n",
    "Then, the next step is to Get the SubGraph from it, in NebulaGraph, it's actuall a query like this:\n",
    "\n",
    "```SQL\n",
    "GET SUBGRAPH WITH PROP FROM 2048 YIELD VERTICES AS nodes, EDGES AS relationships;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6bcac-a3a9-4e55-86ab-209bf3014ec0",
   "metadata": {},
   "source": [
    "To Better understand what this SubGraph is, we could see them from NebulaGraph Studio:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/1651790/182024973-e92c8430-208c-4a0a-bf31-1b7a197d9241.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/1651790/182025007-634b0098-61a6-4c0c-b061-7f2f74b9755c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b96775-69d4-4c80-b083-542afae56e78",
   "metadata": {},
   "source": [
    "In fact, @HarrisChu had been working on bringing the Graph-Notebook open-sourced by AWS to enable even better way to query NebulaGraph in Notebook.\n",
    "\n",
    "https://github.com/HarrisChu/nebula-opencypher-adapter\n",
    "\n",
    "First, let's install the graph-notebook as extension of Jupyter Notebook.\n",
    "\n",
    "```bash\n",
    "# pin specific versions of required dependencies\n",
    "pip install rdflib==5.0.0\n",
    "pip install markupsafe==2.0.1\n",
    "\n",
    "# install the package\n",
    "pip install graph-notebook\n",
    "```\n",
    "\n",
    "Then, let's setup @HarrisChu's opencypher proxy to \"man-in-the-middle\" our graphD on space `yelp`:\n",
    "\n",
    "```bash\n",
    "./nebula-opencypher-adapter -a graphd:9669 -s yelp --port 8001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a93d2c-65f1-4839-986b-bd72c9d90d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext graph_notebook.magics\n",
    "# host value is dgl is my current hostname as it's running as a container named dgl\n",
    "%%graph_notebook_config\n",
    "{\n",
    "  \"host\": \"dgl\",\n",
    "  \"port\": 8001,\n",
    "  \"ssl\": false,\n",
    "  \"gremlin\": {\n",
    "    \"traversal_source\": \"g\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828521f-ce12-4aa9-bfcc-6ec399679bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i.e. we query subgraph of three nodes like:\n",
    "%%oc\n",
    "GET SUBGRAPH FROM 2048 YIELD VERTICES AS nodes, EDGES AS relationships;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc882822-e485-4ccb-b567-19f46ed17a5d",
   "metadata": {},
   "source": [
    "And you should see it as:\n",
    "![graph-notebook-demo](https://user-images.githubusercontent.com/1651790/182779634-1608bd60-87dc-4a73-8483-f8871c7ae1e5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32366bb7-75a0-44d5-8000-c3be55eb6067",
   "metadata": {},
   "source": [
    "## The code of Get SubGraph for a new record\n",
    "\n",
    "In the Inference API webservice, we should:\n",
    "a. Query NebulaGraph for SubGraph\n",
    "b. Construct it as a DGL Object\n",
    "\n",
    "For step a, we used Nebula-Python, the Python SDK/Client for NebulaGraph, with `execute_json()` the SubGraph was fetched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737b617d-09a6-43f6-8223-4e34dce587de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get SUBGRAPH of one node\n",
    "\n",
    "import json\n",
    "from torch import tensor\n",
    "from dgl import DGLHeteroGraph, heterograph\n",
    "\n",
    "from nebula3.gclient.net import ConnectionPool\n",
    "from nebula3.Config import Config\n",
    "\n",
    "config = Config()\n",
    "config.max_connection_pool_size = 2\n",
    "connection_pool = ConnectionPool()\n",
    "connection_pool.init([('graphd', 9669)], config)\n",
    "\n",
    "vertex_id = 2048\n",
    "client = connection_pool.get_session('root', 'nebula')\n",
    "r = client.execute_json(\n",
    "    \"USE yelp;\"\n",
    "    f\"GET SUBGRAPH WITH PROP 2 STEPS FROM {vertex_id} YIELD VERTICES AS nodes, EDGES AS relationships;\")\n",
    "\n",
    "r = json.loads(r)\n",
    "data = r.get('results', [{}])[0].get('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6a770d-749f-4260-8217-3c2d154d70e1",
   "metadata": {},
   "source": [
    "Then, for step b, we leverage the `heterograph()` to load it from an expected `data_dict`, ref: https://docs.dgl.ai/en/0.9.x/generated/dgl.heterograph.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5eb0ede-3ee8-4f2c-a7d7-4c8c8cd34a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create node and nodedata\n",
    "node_id_map = {} # key: vertex id in NebulaGraph, value: node id in dgl_graph\n",
    "node_idx = 0\n",
    "features = [[] for _ in range(32)] + [[]]\n",
    "for i in range(len(data)):\n",
    "    for index, node in enumerate(data[i]['meta'][0]):\n",
    "        nodeid = data[i]['meta'][0][index]['id']\n",
    "        if nodeid not in node_id_map:\n",
    "            node_id_map[nodeid] = node_idx\n",
    "            node_idx += 1\n",
    "            for f in range(32):\n",
    "                features[f].append(data[i]['row'][0][index][f\"review.f{f}\"])\n",
    "            features[32].append(data[i]['row'][0][index]['review.is_fraud'])\n",
    "\n",
    "\n",
    "rur_start, rur_end, rsr_start, rsr_end, rtr_start, rtr_end = [], [], [], [], [], []\n",
    "for i in range(len(data)):\n",
    "    for edge in data[i]['meta'][1]:\n",
    "        edge = edge['id']\n",
    "        if edge['name'] == 'shares_user_with':\n",
    "            rur_start.append(node_id_map[edge['src']])\n",
    "            rur_end.append(node_id_map[edge['dst']])\n",
    "        elif edge['name'] == 'shares_restaurant_rating_with':\n",
    "            rsr_start.append(node_id_map[edge['src']])\n",
    "            rsr_end.append(node_id_map[edge['dst']])\n",
    "        elif edge['name'] == 'shares_restaurant_in_one_month_with':\n",
    "            rtr_start.append(node_id_map[edge['src']])\n",
    "            rtr_end.append(node_id_map[edge['dst']])\n",
    "\n",
    "data_dict = {}\n",
    "if rur_start:\n",
    "    data_dict[('review', 'shares_user_with', 'review')] = tensor(rur_start), tensor(rur_end)\n",
    "if rsr_start:\n",
    "    data_dict[('review', 'shares_restaurant_rating_with', 'review')] = tensor(rsr_start), tensor(rsr_end)\n",
    "if rtr_start:\n",
    "    data_dict[('review', 'shares_restaurant_in_one_month_with', 'review')] = tensor(rtr_start), tensor(rtr_end)\n",
    "\n",
    "# construct a dgl_graph\n",
    "dgl_graph: DGLHeteroGraph = heterograph(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb97475-6798-43ee-aa9c-6c1fc3df4921",
   "metadata": {},
   "source": [
    "Then, add node features, too\n",
    "\n",
    "> Note, it's worth to be mentioned that `node_id_map` is mapping NebulaGraph Vertex_ID to node_id in DGL object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463a0608-cb11-4ae4-81c7-614c587d79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load node features to dgl_graph\n",
    "for i in range(32):\n",
    "    dgl_graph.ndata[f\"f{i}\"] = tensor(features[i])\n",
    "dgl_graph.ndata['label'] = tensor(features[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9137c-d080-4c1a-9778-6b75463d8edd",
   "metadata": {},
   "source": [
    "And we need to transform it to homogeneous graph as we did during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00edae7c-0238-446e-88dc-a998597f690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# to homogeneous graph\n",
    "features = []\n",
    "for i in range(32):\n",
    "    features.append(dgl_graph.ndata[f\"f{i}\"])\n",
    "\n",
    "dgl_graph.ndata['feat'] = torch.stack(features, dim=1)\n",
    "\n",
    "dgl_graph.edges['shares_restaurant_in_one_month_with'].data['he'] = torch.ones(\n",
    "    dgl_graph.number_of_edges('shares_restaurant_in_one_month_with'), dtype=torch.float32)\n",
    "dgl_graph.edges['shares_restaurant_rating_with'].data['he'] = torch.full(\n",
    "    (dgl_graph.number_of_edges('shares_restaurant_rating_with'),), 2, dtype=torch.float32)\n",
    "dgl_graph.edges['shares_user_with'].data['he'] = torch.full(\n",
    "    (dgl_graph.number_of_edges('shares_user_with'),), 4, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# heterogeneous graph to heterogeneous graph, keep ndata and edata\n",
    "import dgl\n",
    "hg = dgl.to_homogeneous(dgl_graph, edata=['he'], ndata=['feat', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c2787-d27f-4faf-af1e-d5b674f3464a",
   "metadata": {},
   "source": [
    "## The Inference API\n",
    "\n",
    "Then here we finaly can build our `do_inference()` API, where the predict happens with the Model we trained and the SubGraph of any new record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d251c3-1549-42dc-9e06-5184b4366aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference(device, graph, node_idx, model, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model.inference(graph, device, batch_size) # pred in buffer_device\n",
    "        return pred[node_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78cb847-5201-4290-b551-9a95547a068b",
   "metadata": {},
   "source": [
    "Let's call it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953f04d1-bd13-445e-9142-f2360b568890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as MF\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from dgl.data import FraudDataset\n",
    "from dgl.dataloading import DataLoader, NeighborSampler, MultiLayerFullNeighborSampler\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from dgl import function as fn\n",
    "from dgl.utils import check_eq_shape, expand_as_pair\n",
    "\n",
    "import json\n",
    "from torch import tensor\n",
    "from dgl import DGLHeteroGraph, heterograph\n",
    "from dgl import function as fn\n",
    "from dgl.utils import check_eq_shape, expand_as_pair\n",
    "\n",
    "class SAGEConv(dglnn.SAGEConv):\n",
    "    def forward(self, graph, feat, edge_weight=None):\n",
    "        r\"\"\"\n",
    "\n",
    "        Description\n",
    "        -----------\n",
    "        Compute GraphSAGE layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : DGLGraph\n",
    "            The graph.\n",
    "        feat : torch.Tensor or pair of torch.Tensor\n",
    "            If a torch.Tensor is given, it represents the input feature of shape\n",
    "            :math:`(N, D_{in})`\n",
    "            where :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.\n",
    "            If a pair of torch.Tensor is given, the pair must contain two tensors of shape\n",
    "            :math:`(N_{in}, D_{in_{src}})` and :math:`(N_{out}, D_{in_{dst}})`.\n",
    "        edge_weight : torch.Tensor, optional\n",
    "            Optional tensor on the edge. If given, the convolution will weight\n",
    "            with regard to the message.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output feature of shape :math:`(N_{dst}, D_{out})`\n",
    "            where :math:`N_{dst}` is the number of destination nodes in the input graph,\n",
    "            :math:`D_{out}` is the size of the output feature.\n",
    "        \"\"\"\n",
    "        self._compatibility_check()\n",
    "        with graph.local_scope():\n",
    "            if isinstance(feat, tuple):\n",
    "                feat_src = self.feat_drop(feat[0])\n",
    "                feat_dst = self.feat_drop(feat[1])\n",
    "            else:\n",
    "                feat_src = feat_dst = self.feat_drop(feat)\n",
    "                if graph.is_block:\n",
    "                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
    "            msg_fn = fn.copy_src('h', 'm')\n",
    "            if edge_weight is not None:\n",
    "                assert edge_weight.shape[0] == graph.number_of_edges()\n",
    "                graph.edata['_edge_weight'] = edge_weight\n",
    "                msg_fn = fn.u_mul_e('h', '_edge_weight', 'm')\n",
    "\n",
    "            h_self = feat_dst\n",
    "\n",
    "            # Handle the case of graphs without edges\n",
    "            if graph.number_of_edges() == 0:\n",
    "                graph.dstdata['neigh'] = torch.zeros(\n",
    "                    feat_dst.shape[0], self._in_src_feats).to(feat_dst)\n",
    "\n",
    "            # Determine whether to apply linear transformation before message passing A(XW)\n",
    "            lin_before_mp = self._in_src_feats > self._out_feats\n",
    "\n",
    "            # Message Passing\n",
    "            if self._aggre_type == 'mean':\n",
    "                graph.srcdata['h'] = self.fc_neigh(feat_src) if lin_before_mp else feat_src\n",
    "                # graph.update_all(msg_fn, fn.mean('m', 'neigh'))\n",
    "                #########################################################################\n",
    "                # consdier datatype with different weight, g.edata['he'] as weight here\n",
    "                g.update_all(fn.u_mul_e('h', 'he', 'm'), fn.mean('m', 'h'))\n",
    "                #########################################################################\n",
    "                h_neigh = graph.dstdata['neigh']\n",
    "                if not lin_before_mp:\n",
    "                    h_neigh = self.fc_neigh(h_neigh)\n",
    "            elif self._aggre_type == 'gcn':\n",
    "                check_eq_shape(feat)\n",
    "                graph.srcdata['h'] = self.fc_neigh(feat_src) if lin_before_mp else feat_src\n",
    "                if isinstance(feat, tuple):  # heterogeneous\n",
    "                    graph.dstdata['h'] = self.fc_neigh(feat_dst) if lin_before_mp else feat_dst\n",
    "                else:\n",
    "                    if graph.is_block:\n",
    "                        graph.dstdata['h'] = graph.srcdata['h'][:graph.num_dst_nodes()]\n",
    "                    else:\n",
    "                        graph.dstdata['h'] = graph.srcdata['h']\n",
    "                graph.update_all(msg_fn, fn.sum('m', 'neigh'))\n",
    "                graph.update_all(fn.copy_e('he', 'm'), fn.sum('m', 'neigh'))\n",
    "                # divide in_degrees\n",
    "                degs = graph.in_degrees().to(feat_dst)\n",
    "                h_neigh = (graph.dstdata['neigh'] + graph.dstdata['h']) / (degs.unsqueeze(-1) + 1)\n",
    "                if not lin_before_mp:\n",
    "                    h_neigh = self.fc_neigh(h_neigh)\n",
    "            elif self._aggre_type == 'pool':\n",
    "                graph.srcdata['h'] = F.relu(self.fc_pool(feat_src))\n",
    "                graph.update_all(msg_fn, fn.max('m', 'neigh'))\n",
    "                graph.update_all(fn.copy_e('he', 'm'), fn.max('m', 'neigh'))\n",
    "                h_neigh = self.fc_neigh(graph.dstdata['neigh'])\n",
    "            elif self._aggre_type == 'lstm':\n",
    "                graph.srcdata['h'] = feat_src\n",
    "                graph.update_all(msg_fn, self._lstm_reducer)\n",
    "                h_neigh = self.fc_neigh(graph.dstdata['neigh'])\n",
    "            else:\n",
    "                raise KeyError('Aggregator type {} not recognized.'.format(self._aggre_type))\n",
    "\n",
    "            # GraphSAGE GCN does not require fc_self.\n",
    "            if self._aggre_type == 'gcn':\n",
    "                rst = h_neigh\n",
    "            else:\n",
    "                rst = self.fc_self(h_self) + h_neigh\n",
    "\n",
    "            # bias term\n",
    "            if self.bias is not None:\n",
    "                rst = rst + self.bias\n",
    "\n",
    "            # activation\n",
    "            if self.activation is not None:\n",
    "                rst = self.activation(rst)\n",
    "            # normalization\n",
    "            if self.norm is not None:\n",
    "                rst = self.norm(rst)\n",
    "            return rst\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_size, hid_size, out_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # three-layer GraphSAGE-mean\n",
    "        self.layers.append(dglnn.SAGEConv(in_size, hid_size, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(hid_size, hid_size, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(hid_size, out_size, 'mean'))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hid_size = hid_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            h = layer(block, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = F.relu(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, device, batch_size):\n",
    "        \"\"\"Conduct layer-wise inference to get all the node embeddings.\"\"\"\n",
    "        feat = g.ndata['feat']\n",
    "        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])\n",
    "        dataloader = DataLoader(\n",
    "                g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,\n",
    "                batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "                num_workers=0)\n",
    "        buffer_device = torch.device('cpu')\n",
    "        pin_memory = (buffer_device != device)\n",
    "\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = torch.empty(\n",
    "                g.num_nodes(), self.hid_size if l != len(self.layers) - 1 else self.out_size,\n",
    "                device=buffer_device, pin_memory=pin_memory)\n",
    "            feat = feat.to(device)\n",
    "            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n",
    "                x = feat[input_nodes]\n",
    "                h = layer(blocks[0], x) # len(blocks) = 1\n",
    "                if l != len(self.layers) - 1:\n",
    "                    h = F.relu(h)\n",
    "                    h = self.dropout(h)\n",
    "                # by design, our output nodes are contiguous\n",
    "                y[output_nodes[0]:output_nodes[-1]+1] = h.to(buffer_device)\n",
    "            feat = y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d0b6ae-6263-4b01-b7a2-6bfec68e716e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1/1 [00:00<00:00, 69.08it/s]\n",
      "100% 1/1 [00:00<00:00, 76.70it/s]\n",
      "100% 1/1 [00:00<00:00, 129.58it/s]\n"
     ]
    }
   ],
   "source": [
    "node_idx = node_id_map[vertex_id]\n",
    "batch_size = 4096\n",
    "device = torch.device('cpu')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = SAGE(32, 256, 2).to(device)\n",
    "model.load_state_dict(torch.load(\"fraud_d.model\"))\n",
    "\n",
    "result = do_inference(device, hg, node_idx, model, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cb66d-4bd1-48a6-a293-f13039d77ae4",
   "metadata": {},
   "source": [
    "Let's see its Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c5fa0e-b240-4ba8-9f89-af8780eb3d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1/1 [00:00<00:00, 102.45it/s]\n",
      "100% 1/1 [00:00<00:00, 101.48it/s]\n",
      "100% 1/1 [00:00<00:00, 124.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_inference(device, graph, nid, model, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model.inference(graph, device, batch_size) # pred in buffer_device\n",
    "        pred = pred[nid]\n",
    "        label = graph.ndata['label'][nid].to(pred.device)\n",
    "        return MF.accuracy(pred, label)\n",
    "\n",
    "node_idx = torch.tensor(list(node_id_map.values()))\n",
    "acc = test_inference(device, hg, node_idx, model, batch_size=4096)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
