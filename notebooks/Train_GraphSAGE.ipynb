{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4ddb97-eee9-4a14-a6a6-7e086b9ee0b0",
   "metadata": {},
   "source": [
    "# Model Training on NebulaGraph\n",
    "<img style=\"float: middle;\" src=\"https://user-images.githubusercontent.com/1651790/182063699-f2f982e0-1dc4-437a-b509-f6247feb475b.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc18097-1780-454b-95f5-f39345a555a7",
   "metadata": {},
   "source": [
    "# Prepare for Playground\n",
    "\n",
    "Now we are setting up a NebulaGraph Playground with a Graph of Yelp-Fraud being loaded.\n",
    "\n",
    "> Data downloader and importing are located [here](https://github.com/wey-gu/nebulagraph-yelp-frauddetection)\n",
    "\n",
    "```bash\n",
    "# Deploy NebulaGraph Cluster\n",
    "curl -fsSL nebula-up.siwei.io/install.sh | bash\n",
    "\n",
    "# Clone the data downloader Repo\n",
    "git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection && cd nebulagraph-yelp-frauddetection\n",
    "\n",
    "# Run downloader\n",
    "python3 -m pip install -r requirements.txt\n",
    "python3 data_download.py\n",
    "\n",
    "# Ingest to NebulaGraph Cluster\n",
    "docker run --rm -ti \\\n",
    " --network=nebula-net \\\n",
    " -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\\n",
    " -v ${PWD}/data:/root \\\n",
    " vesoft/nebula-importer:v3.1.0 \\\n",
    " --config /root/importer.yaml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2832b-d2b2-4463-95b2-d8b2399c0e83",
   "metadata": {},
   "source": [
    "## Load Graph in NebulaGraph Database into DGL\n",
    "\n",
    "We are leveraging [Nebula-DGL](https://github.com/wey-gu/nebula-dgl) here.\n",
    "\n",
    "> You could follow this procedure to setup the Jupyter Playground with connection to our NebulaGraph Playground\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/wey-gu/nebula-dgl.git\n",
    "cd nebula-dgl\n",
    "\n",
    "# Run Jupyter in a Container with network: `nebula-net`\n",
    "\n",
    "docker run -it --name dgl -p 8888:8888 --network nebula-net \\\n",
    "    -v \"$PWD\":/home/jovyan/work jupyter/datascience-notebook \\\n",
    "    start-notebook.sh --NotebookApp.token='nebulagraph'\n",
    "```\n",
    "\n",
    "Access the notebook at http://localhost:8888/lab/tree/work?token=nebulagraph and create a new notebook.\n",
    "\n",
    "```bash\n",
    "# change directory to nebula-dgl for installation\n",
    "cd work\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ec214b2-2fc5-4ff0-846c-be30db5bba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install git+https://github.com/vesoft-inc/nebula-python.git@8c328c534413b04ccecfd42e64ce6491e09c6ca8\n",
    "#!python3 -m pip install .\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as MF\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from dgl.data import FraudDataset\n",
    "from dgl.dataloading import DataLoader, NeighborSampler, MultiLayerFullNeighborSampler\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from dgl import function as fn\n",
    "from dgl.utils import check_eq_shape, expand_as_pair\n",
    "\n",
    "import json\n",
    "from torch import tensor\n",
    "from dgl import DGLHeteroGraph, heterograph\n",
    "\n",
    "from nebula3.gclient.net import ConnectionPool\n",
    "from nebula3.Config import Config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607e4f2-ecd8-4b8d-a713-38baa6e1f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import yaml\n",
    "\n",
    "from nebula_dgl import NebulaLoader\n",
    "\n",
    "# load feature_mapper from yaml file\n",
    "#with open('nebulagraph_yelp_dgl_mapper.yaml', 'r') as f:\n",
    "#    feature_mapper = yaml.safe_load(f)\n",
    "\n",
    "# load feature_mapper from URL\n",
    "mapper_url = \"https://raw.githubusercontent.com/wey-gu/nebulagraph-yelp-frauddetection/main/nebulagraph_yelp_dgl_mapper.yaml\"\n",
    "f = urllib.request.urlopen(mapper_url)\n",
    "feature_mapper = yaml.safe_load(f)\n",
    "\n",
    "nebula_config = {\n",
    "    \"graph_hosts\": [\n",
    "                ('graphd', 9669),\n",
    "                ('graphd1', 9669),\n",
    "                ('graphd2', 9669)\n",
    "            ],\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"nebula\",\n",
    "}\n",
    "\n",
    "nebula_loader = NebulaLoader(nebula_config, feature_mapper)\n",
    "g = nebula_loader.load()\n",
    "# This will take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a235cb-b065-4d31-b11d-5fd5170fd4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When needed, we could save and load the graph, too.\n",
    "# from dgl.data.utils import save_graphs, load_graphs\n",
    "# save_graphs(\"./data.bin\", [g])\n",
    "# g = load_graphs(\"./data.bin\")[0][0]\n",
    "\n",
    "g = g.to('cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38790cc-f3b2-4470-a746-36e3712245de",
   "metadata": {},
   "source": [
    "## Split Data from Graph\n",
    "\n",
    "We need to split the Graph for training into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd06bbd-73f1-4f48-acbe-68f613d66de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# features are g.ndata['f0'], g.ndata['f1'], g.ndata['f2'], ... g.ndata['f31']\n",
    "# label is in g.ndata['is_fraud']\n",
    "\n",
    "# concatenate all features\n",
    "features = []\n",
    "for i in range(32):\n",
    "    features.append(g.ndata['f' + str(i)])\n",
    "\n",
    "g.ndata['feat'] = torch.stack(features, dim=1)\n",
    "g.ndata['label'] = g.ndata['is_fraud']\n",
    "# numpy array as an index of range n\n",
    "\n",
    "idx = torch.tensor(np.arange(g.number_of_nodes()), device=device, dtype=torch.int64)\n",
    "\n",
    "# split based on value distribution of label: the property \"is_fraud\", which is a binary variable.\n",
    "X_train_and_val_idx, X_test_idx, y_train_and_val, y_test = train_test_split(\n",
    "    idx, g.ndata['is_fraud'], test_size=0.2, random_state=42, stratify=g.ndata['is_fraud'])\n",
    "\n",
    "# split train and val\n",
    "X_train_idx, X_val_idx, y_train, y_val = train_test_split(\n",
    "    X_train_and_val_idx, y_train_and_val, test_size=0.2, random_state=42, stratify=y_train_and_val)\n",
    "\n",
    "# list of index to mask\n",
    "train_mask = torch.zeros(g.number_of_nodes(), dtype=torch.bool)\n",
    "train_mask[X_train_idx] = True\n",
    "\n",
    "val_mask = torch.zeros(g.number_of_nodes(), dtype=torch.bool)\n",
    "val_mask[X_val_idx] = True\n",
    "\n",
    "test_mask = torch.zeros(g.number_of_nodes(), dtype=torch.bool)\n",
    "test_mask[X_test_idx] = True\n",
    "\n",
    "g.ndata['train_mask'] = train_mask\n",
    "g.ndata['val_mask'] = val_mask\n",
    "g.ndata['test_mask'] = test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72f648-f547-43d1-82e3-768babd75a32",
   "metadata": {},
   "source": [
    "## Transform the Heterogeneous Graph to Homogeneous Graph\n",
    "\n",
    "Vanilla GraphSAGE is designed to handle Homogeneous only.\n",
    "\n",
    "Now, Yelp dataset comes with one type of node and three types of edges, we could make the type of edge as edge features, it could be a (3-1) 2-D feature or one bit-wise feature:\n",
    "\n",
    "```yaml\n",
    "shares_restaurant_in_one_month_with: 1, b\"001\"\n",
    "shares_restaurant_rating_with: 2, b\"010\"\n",
    "shares_user_with: 4, b\"100\"\n",
    "```\n",
    "\n",
    "> Note after the transformation, `hg.edata['_TYPE']` comes with [0, 1, 2] referring to https://docs.dgl.ai/en/0.9.x/generated/dgl.to_homogeneous.html , we dont want this as 0 could not be used as an edge weight, we'll see why later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe32bb9-220d-4c6c-925c-9cb0e9cb9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge types\n",
    "g.etypes\n",
    "\n",
    "# let's set type info. into edge feature\n",
    "g.edges['shares_restaurant_in_one_month_with'].data['he'] = torch.ones(\n",
    "    g.number_of_edges('shares_restaurant_in_one_month_with'), dtype=torch.int64)\n",
    "g.edges['shares_restaurant_rating_with'].data['he'] = torch.full(\n",
    "    (g.number_of_edges('shares_restaurant_rating_with'),), 2, dtype=torch.int64)\n",
    "g.edges['shares_user_with'].data['he'] = torch.full(\n",
    "    (g.number_of_edges('shares_user_with'),), 4, dtype=torch.int64)\n",
    "\n",
    "# check it\n",
    "\n",
    "g.edata['he']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12013936-7bc1-4c57-85c9-970f0a8ec9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref https://discuss.dgl.ai/t/how-to-convert-from-a-heterogeneous-graph-to-a-homogeneous-graph-with-data/2764\n",
    "\n",
    "# transform, keep `he` as `edata`\n",
    "\n",
    "hg = dgl.to_homogeneous(\n",
    "    g,\n",
    "    edata=['he'],\n",
    "    ndata=['feat', 'label', 'train_mask', 'val_mask', 'test_mask'])\n",
    "\n",
    "# ref https://docs.dgl.ai/en/latest/guide/graph-heterogeneous.html?highlight=to_homogeneous#converting-heterogeneous-graphs-to-homogeneous-graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe9e21-7f11-4800-bdc6-97e404452a8c",
   "metadata": {},
   "source": [
    "## Add edge feature to GraphSAGE\n",
    "\n",
    "The vanilla implementation of GraphSAGE doesn't consider edge feature during message passing.\n",
    "\n",
    "There is an example here: https://github.com/dmlc/dgl/tree/master/examples/pytorch/graphsage\n",
    "\n",
    "In our case, we have to override the `forward` function of `SAGEConv` in one of the two ways:\n",
    "\n",
    "a. copy edge feature, too in `update_all()`\n",
    "```diff\n",
    "  graph.update_all(msg_fn, fn.mean('m', 'neigh'))\n",
    "+ graph.update_all(fn.copy_e('he', 'm'), fn.mean('m', 'neigh'))\n",
    "- h_neigh = graph.dstdata['neigh']\n",
    "+ h_neigh = torch.cat((graph.dstdata['neigh'], graph.dstdata['neigh_e'].reshape(-1, 1)), 1)\n",
    "```\n",
    "> Note, besides the above changes, we need to take care of the dimension, too.\n",
    "\n",
    "b. make edge feature `he` into edge weight during the `update_all()`\n",
    "\n",
    "```diff\n",
    "- graph.update_all(msg_fn, fn.mean('m', 'neigh'))\n",
    "+ # consdier datatype with different weight, g.edata['he'] as weight here\n",
    "+ g.update_all(fn.u_mul_e('h', 'he', 'm'), fn.mean('m', 'h'))\n",
    "```\n",
    "\n",
    "And here we will go for option b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d2a78-ed5e-44e5-a9b3-04966a935bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl import function as fn\n",
    "from dgl.utils import check_eq_shape, expand_as_pair\n",
    "\n",
    "class SAGEConv(dglnn.SAGEConv):\n",
    "    def forward(self, graph, feat, edge_weight=None):\n",
    "        r\"\"\"\n",
    "\n",
    "        Description\n",
    "        -----------\n",
    "        Compute GraphSAGE layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : DGLGraph\n",
    "            The graph.\n",
    "        feat : torch.Tensor or pair of torch.Tensor\n",
    "            If a torch.Tensor is given, it represents the input feature of shape\n",
    "            :math:`(N, D_{in})`\n",
    "            where :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.\n",
    "            If a pair of torch.Tensor is given, the pair must contain two tensors of shape\n",
    "            :math:`(N_{in}, D_{in_{src}})` and :math:`(N_{out}, D_{in_{dst}})`.\n",
    "        edge_weight : torch.Tensor, optional\n",
    "            Optional tensor on the edge. If given, the convolution will weight\n",
    "            with regard to the message.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output feature of shape :math:`(N_{dst}, D_{out})`\n",
    "            where :math:`N_{dst}` is the number of destination nodes in the input graph,\n",
    "            :math:`D_{out}` is the size of the output feature.\n",
    "        \"\"\"\n",
    "        self._compatibility_check()\n",
    "        with graph.local_scope():\n",
    "            if isinstance(feat, tuple):\n",
    "                feat_src = self.feat_drop(feat[0])\n",
    "                feat_dst = self.feat_drop(feat[1])\n",
    "            else:\n",
    "                feat_src = feat_dst = self.feat_drop(feat)\n",
    "                if graph.is_block:\n",
    "                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
    "            msg_fn = fn.copy_src('h', 'm')\n",
    "            if edge_weight is not None:\n",
    "                assert edge_weight.shape[0] == graph.number_of_edges()\n",
    "                graph.edata['_edge_weight'] = edge_weight\n",
    "                msg_fn = fn.u_mul_e('h', '_edge_weight', 'm')\n",
    "\n",
    "            h_self = feat_dst\n",
    "\n",
    "            # Handle the case of graphs without edges\n",
    "            if graph.number_of_edges() == 0:\n",
    "                graph.dstdata['neigh'] = torch.zeros(\n",
    "                    feat_dst.shape[0], self._in_src_feats).to(feat_dst)\n",
    "\n",
    "            # Determine whether to apply linear transformation before message passing A(XW)\n",
    "            lin_before_mp = self._in_src_feats > self._out_feats\n",
    "\n",
    "            # Message Passing\n",
    "            if self._aggre_type == 'mean':\n",
    "                graph.srcdata['h'] = self.fc_neigh(feat_src) if lin_before_mp else feat_src\n",
    "                # graph.update_all(msg_fn, fn.mean('m', 'neigh'))\n",
    "                #########################################################################\n",
    "                # consdier datatype with different weight, g.edata['he'] as weight here\n",
    "                g.update_all(fn.u_mul_e('h', 'he', 'm'), fn.mean('m', 'h'))\n",
    "                #########################################################################\n",
    "                h_neigh = graph.dstdata['neigh']\n",
    "                if not lin_before_mp:\n",
    "                    h_neigh = self.fc_neigh(h_neigh)\n",
    "            elif self._aggre_type == 'gcn':\n",
    "                check_eq_shape(feat)\n",
    "                graph.srcdata['h'] = self.fc_neigh(feat_src) if lin_before_mp else feat_src\n",
    "                if isinstance(feat, tuple):  # heterogeneous\n",
    "                    graph.dstdata['h'] = self.fc_neigh(feat_dst) if lin_before_mp else feat_dst\n",
    "                else:\n",
    "                    if graph.is_block:\n",
    "                        graph.dstdata['h'] = graph.srcdata['h'][:graph.num_dst_nodes()]\n",
    "                    else:\n",
    "                        graph.dstdata['h'] = graph.srcdata['h']\n",
    "                graph.update_all(msg_fn, fn.sum('m', 'neigh'))\n",
    "                graph.update_all(fn.copy_e('he', 'm'), fn.sum('m', 'neigh'))\n",
    "                # divide in_degrees\n",
    "                degs = graph.in_degrees().to(feat_dst)\n",
    "                h_neigh = (graph.dstdata['neigh'] + graph.dstdata['h']) / (degs.unsqueeze(-1) + 1)\n",
    "                if not lin_before_mp:\n",
    "                    h_neigh = self.fc_neigh(h_neigh)\n",
    "            elif self._aggre_type == 'pool':\n",
    "                graph.srcdata['h'] = F.relu(self.fc_pool(feat_src))\n",
    "                graph.update_all(msg_fn, fn.max('m', 'neigh'))\n",
    "                graph.update_all(fn.copy_e('he', 'm'), fn.max('m', 'neigh'))\n",
    "                h_neigh = self.fc_neigh(graph.dstdata['neigh'])\n",
    "            elif self._aggre_type == 'lstm':\n",
    "                graph.srcdata['h'] = feat_src\n",
    "                graph.update_all(msg_fn, self._lstm_reducer)\n",
    "                h_neigh = self.fc_neigh(graph.dstdata['neigh'])\n",
    "            else:\n",
    "                raise KeyError('Aggregator type {} not recognized.'.format(self._aggre_type))\n",
    "\n",
    "            # GraphSAGE GCN does not require fc_self.\n",
    "            if self._aggre_type == 'gcn':\n",
    "                rst = h_neigh\n",
    "            else:\n",
    "                rst = self.fc_self(h_self) + h_neigh\n",
    "\n",
    "            # bias term\n",
    "            if self.bias is not None:\n",
    "                rst = rst + self.bias\n",
    "\n",
    "            # activation\n",
    "            if self.activation is not None:\n",
    "                rst = self.activation(rst)\n",
    "            # normalization\n",
    "            if self.norm is not None:\n",
    "                rst = self.norm(rst)\n",
    "            return rst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23eecf5-8370-41a5-87d8-857e6a028faf",
   "metadata": {},
   "source": [
    "## Defination of the GraphSAGE module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edd05a-a7f8-418f-8aa4-aa94fd676d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_size, hid_size, out_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # three-layer GraphSAGE-mean\n",
    "        self.layers.append(dglnn.SAGEConv(in_size, hid_size, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(hid_size, hid_size, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(hid_size, out_size, 'mean'))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hid_size = hid_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            h = layer(block, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = F.relu(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, device, batch_size):\n",
    "        \"\"\"Conduct layer-wise inference to get all the node embeddings.\"\"\"\n",
    "        feat = g.ndata['feat']\n",
    "        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])\n",
    "        dataloader = DataLoader(\n",
    "                g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,\n",
    "                batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "                num_workers=0)\n",
    "        buffer_device = torch.device('cpu')\n",
    "        pin_memory = (buffer_device != device)\n",
    "\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = torch.empty(\n",
    "                g.num_nodes(), self.hid_size if l != len(self.layers) - 1 else self.out_size,\n",
    "                device=buffer_device, pin_memory=pin_memory)\n",
    "            feat = feat.to(device)\n",
    "            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n",
    "                x = feat[input_nodes]\n",
    "                h = layer(blocks[0], x) # len(blocks) = 1\n",
    "                if l != len(self.layers) - 1:\n",
    "                    h = F.relu(h)\n",
    "                    h = self.dropout(h)\n",
    "                # by design, our output nodes are contiguous\n",
    "                y[output_nodes[0]:output_nodes[-1]+1] = h.to(buffer_device)\n",
    "            feat = y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec295f-5142-42cd-b3b2-ef415e95a5fc",
   "metadata": {},
   "source": [
    "## Define train, inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbd9dc-287b-4504-b3b7-771eb85bbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graph, dataloader):\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    y_hats = []\n",
    "    for it, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            x = blocks[0].srcdata['feat']\n",
    "            ys.append(blocks[-1].dstdata['label'])\n",
    "            y_hats.append(model(blocks, x))\n",
    "    return MF.accuracy(torch.cat(y_hats), torch.cat(ys))\n",
    "\n",
    "def layerwise_infer(device, graph, nid, model, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model.inference(graph, device, batch_size) # pred in buffer_device\n",
    "        pred = pred[nid]\n",
    "        label = graph.ndata['label'][nid].to(pred.device)\n",
    "        return MF.accuracy(pred, label)\n",
    "\n",
    "def train(device, g, model, train_idx, val_idx):\n",
    "    # create sampler & dataloader\n",
    "    sampler = NeighborSampler([10, 10, 10],  # fanout for [layer-0, layer-1, layer-2]\n",
    "                              prefetch_node_feats=['feat'],\n",
    "                              prefetch_labels=['label'])\n",
    "    use_uva = False\n",
    "    train_dataloader = DataLoader(g, train_idx, sampler, device=device,\n",
    "                                  batch_size=1024, shuffle=True,\n",
    "                                  drop_last=False, num_workers=0,\n",
    "                                  use_uva=use_uva)\n",
    "\n",
    "    val_dataloader = DataLoader(g, val_idx, sampler, device=device,\n",
    "                                batch_size=1024, shuffle=True,\n",
    "                                drop_last=False, num_workers=0,\n",
    "                                use_uva=use_uva)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for it, (input_nodes, output_nodes, blocks) in enumerate(train_dataloader):\n",
    "            x = blocks[0].srcdata['feat']\n",
    "            y = blocks[-1].dstdata['label']\n",
    "            y_hat = model(blocks, x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        acc = evaluate(model, g, val_dataloader)\n",
    "        print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} \"\n",
    "              .format(epoch, total_loss / (it+1), acc.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73780004-921b-459a-bacf-e35f0ae19919",
   "metadata": {},
   "source": [
    "## Train and Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac1c79-697f-4c30-a1ed-1bbd6eccddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GraphSAGE model\n",
    "in_size = hg.ndata['feat'].shape[1]\n",
    "out_size = 2\n",
    "model = SAGE(in_size, 256, out_size).to(device)\n",
    "\n",
    "# model training\n",
    "print('Training...')\n",
    "train(device, hg, model, X_train_idx, X_val_idx)\n",
    "\n",
    "# test the model\n",
    "print('Testing...')\n",
    "\n",
    "acc = layerwise_infer(device, hg, X_test_idx, model, batch_size=4096)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f47ca-535e-4944-a4be-1857b10a3609",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "We could save the model into file and load it in other processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074020fc-62cb-4668-82cc-0f0bdd9af823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"fraud_d.model\")\n",
    "\n",
    "# load model, think of it's on our Fraud_detection_API_backend_service\n",
    "device = torch.device('cpu')\n",
    "model = SAGE(32, 256, 2).to(device)\n",
    "model.load_state_dict(torch.load(\"fraud_d.model\"))\n",
    "\n",
    "# think of this is a query, I know, hg is not, but just an example.\n",
    "layerwise_infer(device, hg, X_test_idx, model, batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabb904-51af-4bf9-b952-c4bff91e7419",
   "metadata": {},
   "source": [
    "## Inductive Learning\n",
    "\n",
    "The reason I choose to use GraphSAGE was due to it's a very simple model for Inductive Learning, which means we could predict **Graph with NEW nodes**, so that the Fraud Detection system could be done online.\n",
    "\n",
    "But in above example, the data we were verifying the model isn't new, to prove that coudl be done, here is the example on redo the dataset split:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f678c4d-20d3-4dcb-b8bf-75b059eb3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inductive Learning, our test dataset are new nodes and new edges\n",
    "hg_train = hg.subgraph(torch.cat([X_train_idx, X_val_idx]))\n",
    "\n",
    "# model training\n",
    "print('Training...')\n",
    "train(device, hg_train, model, torch.arange(X_train_idx.shape[0]), torch.arange(X_train_idx.shape[0], hg_train.num_nodes()))\n",
    "\n",
    "# test the model\n",
    "print('Testing...')\n",
    "\n",
    "hg_test = hg.subgraph(torch.cat([X_test_idx]))\n",
    "\n",
    "sg_X_test_idx = torch.arange(hg_test.num_nodes())\n",
    "\n",
    "acc = layerwise_infer(device, hg_test, sg_X_test_idx, model, batch_size=4096)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881791e-dff9-48d9-a64d-837de1fbeb70",
   "metadata": {},
   "source": [
    "From above new dataset spliting, we could see the graph: `hg_test` to be predicted are brand new data with no intersection with the training data/graph.\n",
    "\n",
    "And it worked fine, tooüòÅ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37042321-b942-4f3c-9c12-2d048abe33b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
